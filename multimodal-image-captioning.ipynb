{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dd490-a5e6-4058-a4d8-f10e9c0e4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24de44f5-a2f8-48ba-9f8f-bcdd722eee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2c5c71-483b-46d6-a5dd-e97c241d5642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image,caption</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A child in a pink dr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A girl going into a ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A little girl climbi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A little girl climbi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  caption\n",
       "0                                      image,caption      NaN\n",
       "1  1000268201_693b08cb0e.jpg,A child in a pink dr...      NaN\n",
       "2  1000268201_693b08cb0e.jpg,A girl going into a ...      NaN\n",
       "3  1000268201_693b08cb0e.jpg,A little girl climbi...      NaN\n",
       "4  1000268201_693b08cb0e.jpg,A little girl climbi...      NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "image_folder = \"data/images/\"\n",
    "captions_file = \"data/captions.txt\"\n",
    "\n",
    "# Read captions\n",
    "captions_data = pd.read_csv(captions_file, delimiter='\\t', names=['image', 'caption'])\n",
    "captions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073ec20b-2291-45da-a67f-b32392ab1f41",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Build vocabulary\u001b[39;00m\n\u001b[0;32m     29\u001b[0m vocab \u001b[38;5;241m=\u001b[39m Vocabulary(freq_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m vocab\u001b[38;5;241m.\u001b[39mbuild_vocab(captions_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mVocabulary.build_vocab\u001b[1;34m(self, sentence_list)\u001b[0m\n\u001b[0;32m     15\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentence_list:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(sentence):\n\u001b[0;32m     18\u001b[0m         frequencies[word] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m frequencies[word] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq_threshold:\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mVocabulary.tokenizer\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m, text\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                frequencies[word] +=1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocab(captions_data['caption'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8fb3a-f92e-413f-b707-d11d3ddf1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_folder, vocab, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.img_folder = img_folder\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image']\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        \n",
    "        image = Image.open(os.path.join(self.img_folder, img_name)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        numericalized_caption = [vocab.stoi[\"<SOS>\"]] + vocab.numericalize(caption) + [vocab.stoi[\"<EOS>\"]]\n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(captions_data, image_folder, vocab, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a689f45-61ea-476f-91c8-e1f88c567468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
