{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dd490-a5e6-4058-a4d8-f10e9c0e4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24de44f5-a2f8-48ba-9f8f-bcdd722eee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2c5c71-483b-46d6-a5dd-e97c241d5642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image,caption</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A child in a pink dr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A girl going into a ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A little girl climbi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg,A little girl climbi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  caption\n",
       "0                                      image,caption      NaN\n",
       "1  1000268201_693b08cb0e.jpg,A child in a pink dr...      NaN\n",
       "2  1000268201_693b08cb0e.jpg,A girl going into a ...      NaN\n",
       "3  1000268201_693b08cb0e.jpg,A little girl climbi...      NaN\n",
       "4  1000268201_693b08cb0e.jpg,A little girl climbi...      NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths\n",
    "image_folder = \"data/images/\"\n",
    "captions_file = \"data/captions.txt\"\n",
    "\n",
    "# Read captions\n",
    "captions_data = pd.read_csv(captions_file, delimiter='\\t', names=['image', 'caption'])\n",
    "captions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76638d25-9f13-4277-a278-d1414a4896fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokenized_text]\n",
    "\n",
    "# Clean captions and build vocabulary\n",
    "captions_data = captions_data.dropna(subset=['caption'])\n",
    "captions_data['caption'] = captions_data['caption'].astype(str)\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocab(captions_data['caption'].tolist())\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab.stoi))\n",
    "print(\"Example tokens:\", list(vocab.stoi.items())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8fb3a-f92e-413f-b707-d11d3ddf1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_folder, vocab, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.img_folder = img_folder\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.df.iloc[idx]['image']\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        \n",
    "        image = Image.open(os.path.join(self.img_folder, img_name)).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        numericalized_caption = [vocab.stoi[\"<SOS>\"]] + vocab.numericalize(caption) + [vocab.stoi[\"<EOS>\"]]\n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(captions_data, image_folder, vocab, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823339b-7cb4-4813-9958-caf8e0ca9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a689f45-61ea-476f-91c8-e1f88c567468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]  # Remove FC layer\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images).squeeze()\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572efd19-35c1-4f3a-b5f6-458ecc22c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de6d5b-f952-4604-b173-0c9c7a8b2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25354a69-f91c-4e5a-bfea-0682672ed32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "vocab_size = len(vocab.stoi)\n",
    "\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462eab1-7560-457f-bd20-fb0917d95ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(dataloader):\n",
    "        images, captions = zip(*batch)\n",
    "        images = torch.stack(images).to(device)\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "        captions_padded = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=vocab.stoi[\"<PAD>\"]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions_padded[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), captions_padded[:,1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save models\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(encoder.state_dict(), \"models/encoder.pth\")\n",
    "torch.save(decoder.state_dict(), \"models/decoder.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903622b-7fce-45bf-ba2b-65e82d5c3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image, encoder, decoder, vocab, max_len=20):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        image = transform(image).unsqueeze(0).to(device)\n",
    "        feature = encoder(image)\n",
    "        caption = [vocab.stoi[\"<SOS>\"]]\n",
    "        for _ in range(max_len):\n",
    "            inputs = torch.tensor(caption).unsqueeze(0).to(device)\n",
    "            outputs = decoder(feature, inputs)\n",
    "            predicted = outputs.argmax(2)[:,-1].item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab.stoi[\"<EOS>\"]:\n",
    "                break\n",
    "    words = [vocab.itos[idx] for idx in caption[1:-1]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Example\n",
    "img_path = \"data/images/123456.jpg\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "caption = generate_caption(img, encoder, decoder, vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
